# Text Generation with GPT-2

## Prodigy InfoTech â€“ Generative AI Internship (Task-01)

### ðŸ“Œ Task Objective
The objective of this task is to generate coherent and contextually relevant text using a pre-trained GPT-2 model. This task helps in understanding how transformer-based language models work and how prompts influence generated text.

---

### ðŸ›  Tools & Technologies Used
- Python  
- Google Colab  
- Hugging Face Transformers  
- GPT-2 (Pre-trained Model)  
- PyTorch  

---

### ðŸ“‚ Project Description
In this project, a pre-trained GPT-2 model is used to generate text based on a given input prompt. The input text is tokenized and passed to the model, which predicts the next sequence of words based on learned language patterns. The generated output demonstrates how AI models can produce human-like text.

---

### ðŸ”„ Workflow
1. Install required libraries (transformers, torch)  
2. Load the GPT-2 tokenizer and model  
3. Provide an input text prompt  
4. Generate text using the model  
5. Decode and display the generated output  

---

### ðŸ“ Sample Input

Artificial Intelligence is changing the world because


---

### ðŸ“¤ Sample Output
Artificial Intelligence is changing the world because it enables machines to learn from data, automate processes, and improve decision-making across various industries.

---

### ðŸŽ¯ Learning Outcomes
- Understanding of transformer-based language models  
- Practical experience with GPT-2 text generation  
- Knowledge of prompt-based AI text generation  
- Experience using Hugging Face libraries  

---

## ðŸ” Extra Showcase â€“ Text-Generation-with-GPT-2 (Web Application)

As an additional showcase beyond the mandatory task requirements, a **Text Generation Web Application** was developed to demonstrate the practical integration of the GPT-2 model into a real-time web interface.

This showcase highlights how a trained GPT-2 model can be deployed using a backend framework and accessed through a browser-based UI, allowing users to input prompts and receive AI-generated text dynamically.

### âœ¨ Key Highlights
- Real-time text generation through a web interface  
- Backend powered by Python and Flask  
- GPT-2 model integrated using Hugging Face Transformers  
- Clean and minimal user interface  

### ðŸ§° Technologies Used
- Python  
- Flask  
- GPT-2 (Hugging Face Transformers)  
- HTML  
- CSS  

### ðŸ”— Showcase Repository
ðŸ‘‰ **Text-Generation-with-GPT-2**  
https://github.com/padmapriya-a-byte/Text-Generation-with-GPT-2  

> ðŸ“Œ *Note: This web application is created purely as an additional learning and showcase project and is not part of the mandatory Prodigy InfoTech task submission.*

---

### ðŸ“Ž Author
**Padmapriya.V**
